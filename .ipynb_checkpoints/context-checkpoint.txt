CONTEXTE GLOBAL – PROJET E-COMMERCE OLIST (BRÉSIL)

1) CONTEXTE TECHNIQUE

- Projet : analyse de données e-commerce au Brésil (données publiques Olist).
- Fichiers bruts .csv lus depuis : ../donnees/ebrasil
- Tables nettoyées sauvegardées en .parquet dans : ./ecommerce
- Imports principaux : pandas, numpy, matplotlib, seaborn, datetime, etc.
- Utilisation d’un dictionnaire des états brésiliens (sigle → nom complet) pour enrichir les tables clients, vendeurs et géolocalisation.

DICT ETATS BRESILIENS (dictEtats)
AC → Acre
AL → Alagoas
AP → Amapá
AM → Amazonas
BA → Bahia
CE → Ceará
ES → Espírito Santo
GO → Goiás
MA → Maranhão
MT → Mato Grosso
MS → Mato Grosso do Sul
MG → Minas Gerais
PA → Pará
PB → Paraíba
PR → Paraná
PE → Pernambouc
PI → Piauí
RJ → Rio de Janeiro
RN → Rio Grande do Norte
RS → Rio Grande do Sul
RO → Rondônia
RR → Roraima
SC → Santa Catarina
SP → São Paulo
SE → Sergipe
TO → Tocantins
DF → District fédéral


2) TABLES PRINCIPALES (PARQUET)

2.1) TABLE ORDERS – ./ecommerce/orders.parquet
Source : olist_orders_dataset.csv

Colonnes principales :
- order_id : identifiant commande
- customer_id : identifiant client
- purchase_timestamp : datetime (date/heure d’achat)
- approved_at : datetime (date/heure d’approbation)
- delivered_carrier : datetime (prise en charge par le transporteur)
- delivered_customer : datetime (livraison au client)
- estimated_delivery : datetime (date de livraison estimée)
- status : statut de la commande

Dimensions de temps dérivées de purchase_timestamp :
- annee, mois, annee_mois
- jour, annee_jour
- jour_semaine
- trimestre, annee_trimestre
- semaine, annee_semaine
- heure

Durées (en heures, actuellement calculées avec .dt.seconds → tronquées modulo 24h) :
- approuvee = approved_at – purchase_timestamp
- envoyee   = delivered_carrier – purchase_timestamp
- livree    = delivered_customer – purchase_timestamp
- estimee   = estimated_delivery – purchase_timestamp

Transformations :
- Conversion des colonnes brutes de dates en vraies datetime.
- Suppression des anciennes colonnes de dates + order_status.
- Calcul des colonnes de temps et des durées.
- Sauvegarde en Parquet : ./ecommerce/orders.parquet


2.2) TABLE ITEMS – ./ecommerce/items.parquet
Source : olist_order_items_dataset.csv

Colonnes principales :
- order_id
- order_item_id (int)
- product_id
- seller_id
- price (float)
- freight_value (float)
- shipping_limit : datetime (date limite d’expédition)
- limit : durée (en heures) entre la commande et la date limite d’expédition
  (actuellement tronquée modulo 24h via .dt.seconds).

Transformations :
- Jointure avec orders.parquet pour récupérer purchase_timestamp.
- Conversion shipping_limit_date → shipping_limit (datetime).
- Calcul de limit = (shipping_limit – purchase_timestamp) en heures.
- Suppression de shipping_limit_date et purchase_timestamp.
- Sauvegarde : ./ecommerce/items.parquet (gzip, pyarrow).


2.3) TABLE PAYMENTS – ./ecommerce/payments.parquet
Source : olist_order_payments_dataset.csv

Structure logique (après pivot en mémoire) : 1 ligne par commande (order_id), colonnes par type de paiement.
Types : boleto, credit_card, debit_card, not_defined, voucher.

Colonnes principales après pivot :
- int_boleto, int_credit_card, int_debit_card, int_not_defined, int_voucher (int8)
- value_boleto, value_credit_card, value_debit_card, value_not_defined, value_voucher (float)
- value : montant total payé tous moyens confondus.

Transformations :
- Lecture CSV, suppression du préfixe “payment_”.
- pivot_table pour passer d’une ligne par paiement à une ligne par commande.
- Remplacement de “installments” par “int”.
- Cast des colonnes int_* en int8.
- Calcul de value = somme des value_*.

Remarque : dans l’état actuel du notebook, la table brute (une ligne par paiement) est encore celle sauvegardée. Pour sauvegarder la table pivotée :
payments.reset_index().to_parquet('./ecommerce/payments.parquet', ...).


2.4) TABLE REVIEWS – ./ecommerce/reviews.parquet
Source : olist_order_reviews_dataset.csv
But : résumer les avis clients par commande, avec agrégations par score (1 à 5).

Colonnes par score (1 à 5) :
- creation_1 … creation_5 : délai (h) commande → création avis
- answer_1 … answer_5 : délai (h) création avis → réponse
- comment_1 … comment_5 : longueur commentaire (nb de caractères)
- score_1 … score_5 : nombre d’avis de chaque score

Colonnes agrégées globales par commande :
- score : score agrégé (somme pondérée des notes 1–5)
- creation : somme des creation_1…5
- answer  : somme des answer_1…5
- comment : somme des comment_1…5

NB : creation et answer sont actuellement calculées via .dt.seconds et donc tronquées modulo 24h.

Transformations :
- Suppression du préfixe review_ (sauf ids).
- Conversion en datetime : creation_date, answer_timestamp.
- Jointure avec orders.parquet pour récupérer purchase_timestamp.
- Calcul des deltas de temps (en heures).
- Calcul de la longueur de comment_message.
- Agrégations/pivot par order_id et score pour générer *_1…*_5 + colonnes agrégées.
- Sauvegarde : ./ecommerce/reviews.parquet.


2.5) TABLE PRODUCTS – ./ecommerce/products.parquet
Source : olist_products_dataset.csv

Colonnes :
- product_id
- name_lenght
- description_lenght
- photos_qty
- weight_g
- length_cm
- height_cm
- width_cm
- category_name

Transformations :
- Nettoyage / cast des colonnes numériques.
- Gestion des NaN.
- Traduction des catégories en anglais via product_category_name_translation.csv.
- Corrections manuelles, NaN → 'not documented'.
- Renommage de category_name_english en category_name.
- Sauvegarde : ./ecommerce/products.parquet.


2.6) TABLE CUSTOMERS – ./ecommerce/customers.parquet
Source : olist_customers_dataset.csv

Colonnes :
- customer_id
- customer_unique_id
- zip_code (int32) – ex customer_zip_code_prefix
- city
- state (sigle, ex : SP)
- name_state (nom complet de l’état via dictEtats)

Transformations :
- Conversion de customer_zip_code_prefix en int32 + renommage en zip_code.
- Renommage customer_city → city, customer_state → state.
- Ajout de name_state via dictEtats[state].
- Sauvegarde : ./ecommerce/customers.parquet.


2.7) TABLE SELLERS – ./ecommerce/sellers.parquet
Source : olist_sellers_dataset.csv

Colonnes :
- seller_id
- zip_code (int32)
- city
- state (sigle)
- name_state

Transformations :
- Suppression des préfixes seller_ et _prefix.
- Conversion zip_code en int32.
- Ajout de name_state via dictEtats[state].
- Sauvegarde : ./ecommerce/sellers.parquet.


2.8) TABLE GEOLOCATION – ./ecommerce/geolocation.parquet
Source : olist_geolocation_dataset.csv

Colonnes après agrégation :
- zip_code (int32)
- city (first)
- state (nom complet via dictEtats)
- lat_min, lat_max, lat (médiane)
- lng_min, lng_max, lng (médiane)

Transformations :
- Suppression des préfixes geolocation_ et _prefix.
- Remplacement sigles d’état → nom complet via dictEtats.
- Conversion zip_code en int32.
- Groupby zip_code : agrégations min / median / max pour lat et lng, first pour city et state.
- Renommage en lat_min, lat_max, lat, lng_min, lng_max, lng.
- Sauvegarde : ./ecommerce/geolocation.parquet.


3) STRUCTURE GLOBALE DU MODÈLE

Tables de faits :
- orders   : infos commande + temps + délais logistiques
- items    : lignes de commande (produits / vendeurs)
- payments : paiements par commande (modes et montants)
- reviews  : avis clients agrégés par commande

Dimensions :
- customers   : infos clients + localisation
- sellers     : infos vendeurs + localisation
- products    : caractéristiques produits + catégorie
- geolocation : infos géographiques par code postal

Clés de jointure :
- order_id    : orders ↔ items ↔ payments ↔ reviews
- customer_id : orders ↔ customers
- seller_id   : items ↔ sellers
- product_id  : items ↔ products
- zip_code    : customers/sellers ↔ geolocation


4) DATAFRAMES CONSOLIDÉS POUR L’ANALYSE

L’analyse finale repose sur 3 dataFrames consolidés (décrits aussi dans le PDF de consignes) :

4.1) DataFrame 1 – « Analyse des ventes et avis clients »
Schéma logique (d’après le PDF et les schémas d’origine) :
- Basé sur : olist_orders_dataset, olist_order_payments_dataset, olist_order_reviews_dataset,
             olist_order_customer_dataset, olist_geolocation_dataset.
- Clé centrale : order_id (commandes).
- Données : ventes (montants, paiements), délais de livraison, scores d’avis, dimensions client + géographie, temps.

Exemple d’organisation de modèle logique (simplifié) :
- FAIT_VENTES : mesures liées à la commande + paiements + score d’avis.
- DIM_CLIENT / DIM_CLIENT_GEO
- DIM_TEMPS
- DIM_STATUT_COMMANDE
- éventuellement DIM_PAIEMENT et DIM_AVIS (selon le choix du modèle en étoile).


4.2) DataFrame 2 – « Analyse des achats »
Schéma logique (d’après le PDF et le MLD que tu as fourni) :

FACT TABLE : FAIT_ACHATS
- order_id
- order_item_id
- price
- freight_value
- line_revenue
- line_total
- purchase_timestamp
- year
- month

DIMENSIONS :
- PRODUIT : product_id, product_category_name, product_weight_g,
            product_length_cm, product_height_cm, product_width_cm
- CLIENT : customer_id, customer_zip_code_prefix, customer_city,
           customer_state, cust_lat, cust_lng
- VENDEUR : seller_id, seller_zip_code_prefix, seller_city,
            seller_state, sell_lat, sell_lng
- TEMPS : purchase_timestamp, year, month
- STATUT_COMMANDE : order_status

RELATIONS (selon ton MLD) :
- RELIE_PRODUIT  (1,1 FAIT_ACHATS, 0,n PRODUIT)
- RELIE_CLIENT   (1,1 FAIT_ACHATS, 0,n CLIENT)
- RELIE_VENDEUR  (1,1 FAIT_ACHATS, 0,n VENDEUR)
- RELIE_TEMPS    (1,1 FAIT_ACHATS, 0,n TEMPS)
- RELIE_STATUT   (1,1 FAIT_ACHATS, 0,n STATUT_COMMANDE)


4.3) DataFrame 3 – « Analyse des achats (agrégés) et ventes »
Schéma logique (d’après le PDF, les images et ton MLD) :

FACT TABLE : FAIT_VENTES
- order_id
- order_purchase_timestamp
- year
- month
- order_payment_value
- payment_methods_count
- payment_installments_max
- payment_main_type
- payment_types_concat
- review_score
- customer_id
- review_id
- order_status

DIMENSIONS :
- TEMPS : year, month
- AVIS : review_id, review_creation_date, review_answer_timestamp
- CLIENT_GEO : customer_id, customer_zip_code_prefix, customer_city,
               customer_state, cust_lat, cust_lng,
               cust_geo_city, cust_geo_state
- STATUT_COMMANDE : order_status

RELATIONS :
- RELIE_TEMPS   (1,1 FAIT_VENTES, 0,n TEMPS)
- RELIE_AVIS    (1,1 FAIT_VENTES, 0,n AVIS)
- RELIE_CLIENT  (1,1 FAIT_VENTES, 0,n CLIENT_GEO)
- RELIE_STATUT  (1,1 FAIT_VENTES, 0,n STATUT_COMMANDE)

Autre variante d’étoile basée sur FAIT_ACHATS (agrégé) :
FACT TABLE : FAIT_ACHATS (version enrichie)
- order_id
- order_item_id
- price
- freight_value
- line_revenue
- line_total
- order_payment_value
- order_avg_review
- purchase_timestamp
- year
- month

DIMENSIONS :
- PRODUIT : product_id, product_category_name, product_weight_g,
            product_length_cm, product_height_cm, product_width_cm
- VENDEUR : seller_id, seller_zip_code_prefix, seller_city,
            seller_state, sell_lat, sell_lng
- CLIENT : customer_id, customer_zip_code_prefix, customer_city,
           customer_state, cust_lat, cust_lng
- TEMPS : purchase_timestamp, year, month
- STATUT_COMMANDE : order_status

RELATIONS :
- RELIE_PRODUIT  (1,1 FAIT_ACHATS, 0,n PRODUIT)
- RELIE_VENDEUR  (1,1 FAIT_ACHATS, 0,n VENDEUR)
- RELIE_CLIENT   (1,1 FAIT_ACHATS, 0,n CLIENT)
- RELIE_TEMPS    (1,1 FAIT_ACHATS, 0,n TEMPS)
- RELIE_STATUT   (1,1 FAIT_ACHATS, 0,n STATUT_COMMANDE)


5) CONSIGNES DU PROJET (PDF « Projet-E-commerce Brasil-2025 »)

- Les données viennent du dataset Kaggle « Brazilian E-commerce – Olist ».
- L’analyse repose sur les 3 dataFrames consolidés ci-dessus.
- Vous devez construire 2 modèles en étoile (star schemas) à partir de ces données.
  * Identifier pour chaque modèle :
    - les métriques (mesures de fait),
    - les dimensions,
    - les hiérarchies dans chaque dimension (ex. Temps : Année → Trimestre → Mois → Jour ; Géographie : Pays → Région → État → Ville, etc.).

- L’analyse des modèles :
  * Vous devez créer 3 analyses pour chacune des étoiles.
  * Donc au total : 2 modèles × 3 analyses = 6 analyses / 6 notebooks.
  * Chaque analyse :
    - répond à une question claire (problème, hypothèse, recherche),
    - utilise au moins trois hiérarchies,
    - commence par des données très agrégées, puis descend dans les hiérarchies (drill-down),
    - contient au minimum 6–8 graphiques.
  * Chaque analyse est un Notebook Jupyter, sauvegardé aussi en HTML.

- Livrables :
  * Rassembler les 6 notebooks (+ HTML) dans une archive .zip ou .rar.
  * Envoyer à : razvan@bizoi.fr (de préférence via WeTransfer).
  * Date limite d’envoi : avant le 30/11/2025.
  * Préparer une présentation orale de 15–20 minutes pour l’examen du 03/12/2025, basée sur vos modèles étoile, principaux graphiques et insights.
